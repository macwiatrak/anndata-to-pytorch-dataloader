{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "22066479-a09f-4c6a-9da5-2676649ef56c",
   "metadata": {
    "tags": []
   },
   "source": [
    "# From AnnData to Pytorch lightning "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6f92463-9951-46b0-946e-36b10eadb276",
   "metadata": {},
   "source": [
    "This tutorial introduces a custom datamodule, designed to be integrated into [pytorch lighning](https://lightning.ai/) model building pipelines. Under the hood, it build a pytorch dataset from an anndata object and sets up training, validation and test sets ready to interface with Lightning.\n",
    "\n",
    "The tutorial is structured as follows:\n",
    "\n",
    "[1. Example dataset](#load_data)\n",
    "\n",
    "[2. Introducing the datamodule](#dm)\n",
    "\n",
    "[3. Example 1 : simple neural network with pytorch lightning](#mlp)\n",
    "\n",
    "[4. Example 2 : variational autoencoder with pytorch lightnig](#vae)\n",
    "\n",
    "Notes:\n",
    "\n",
    "* this tutorial is adapted from the original AnnLoader [tutorial](https://github.com/scverse/anndata-tutorials/blob/main/annloader.ipynb) by [Sergei Rybakov](https://github.com/koncopd)\n",
    "* Checkout great alternative ways to building interfaces with pytorch, for instance, in [scvi-tools](https://scvi-tools.org/).\n",
    "* Here, we use the [Pyro](http://pyro.ai) framework to simplify the code for a Variational Autoencoder.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8828c195-159d-4b19-b629-fe09d13c4811",
   "metadata": {},
   "source": [
    "## Load packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "57ab6323-109b-4430-96ca-023733df6e08",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/lustre/scratch126/cellgen/team292/mm58/venvs/scgpl_venv/lib/python3.10/site-packages/scvi/_settings.py:63: UserWarning: Since v1.0.0, scvi-tools no longer uses a random seed by default. Run `scvi.settings.seed = 0` to reproduce results from previous versions.\n",
      "  self.seed = seed\n",
      "/lustre/scratch126/cellgen/team292/mm58/venvs/scgpl_venv/lib/python3.10/site-packages/scvi/_settings.py:70: UserWarning: Setting `dl_pin_memory_gpu_training` is deprecated in v1.0 and will be removed in v1.1. Please pass in `pin_memory` to the data loaders instead.\n",
      "  self.dl_pin_memory_gpu_training = (\n",
      "Global seed set to 0\n"
     ]
    }
   ],
   "source": [
    "import gdown\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import scanpy as sc\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from pytorch_lightning import LightningDataModule\n",
    "from torch.utils.data import random_split\n",
    "import torch.nn.functional as F\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "# for scVI-style VAE\n",
    "import pyro\n",
    "import pyro.distributions as dist\n",
    "from scvi.module.base import PyroBaseModuleClass\n",
    "from pyro.nn import PyroModule\n",
    "from typing import Any, Callable, Literal, Optional, Union\n",
    "from inspect import signature\n",
    "\n",
    "import scvi\n",
    "scvi.settings.seed = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "019da188-6083-45d1-9982-702ce3110f7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from anndata_to_pytorch_dataloader.dataset import *\n",
    "from dataset import * # causes error with cuda/nvidia packages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d7ee28f-68cd-4c35-b8a9-b98e8da8c323",
   "metadata": {},
   "source": [
    "<a id='load_data'></a>\n",
    "\n",
    "## Load data\n",
    "\n",
    "The dataset used in this tutorial is the same as the original AnnLoader [tutorial](https://github.com/scverse/anndata-tutorials/blob/main/annloader.ipynb). It has 8 different cell types from 5 different technologies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ed5a591d-8ee5-4aca-bb5a-2b3e3d9491f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found pancreas.h5ad.\n"
     ]
    }
   ],
   "source": [
    "url = 'https://drive.google.com/uc?id=1ehxgfHTsMZXy6YzlFKGJOsBKQ5rrvMnd'\n",
    "output = 'pancreas.h5ad'\n",
    "\n",
    "if not os.path.exists(output):\n",
    "    gdown.download(url, output, quiet=False)\n",
    "    print(f\"File '{output}' downloaded successfully.\")\n",
    "else:\n",
    "    print(f\"Found {output}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d0a6981d-2225-4ce2-8180-304c94176fca",
   "metadata": {},
   "outputs": [],
   "source": [
    "adata = sc.read_h5ad(\"pancreas.h5ad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "66f07b23-aab0-431f-b629-67890fc01abd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert back to raw counts \n",
    "adata.X = adata.raw.X # put raw counts to .X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c9a8dd46-beca-4858-b1db-bc4f3d8798d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "adata.obs['size_factors'] = adata.X.sum(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4dbe1460-679e-403c-8049-370d7e2a9f17",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "adata.var = adata.var.reset_index()\n",
    "adata.var.columns = [\"gene_name\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe94a763-f40a-464e-b1a7-d7ecd0d4bcc1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e4c1167-dab4-4ebe-8a0c-9383c7fe35ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dfe8431c-618e-4961-b429-7279d5388e6a",
   "metadata": {},
   "source": [
    "<a id='dm'></a>\n",
    "\n",
    "## The datamodule\n",
    "\n",
    "The AnnDataset class is designed to create a PyTorch dataset from an Anndata object, allowing seamless integration with PyTorch for deep learning tasks.\n",
    "\n",
    "In PyTorch, a dataset is an abstraction that represents a collection of data samples. Specifically, a PyTorch dataset is a subclass of torch.utils.data.Dataset, and it provides an interface to access and manipulate individual data samples within a collection. It's designed to work seamlessly with PyTorch's data loading utilities, especially DataLoader, which efficiently handles batching, shuffling, and parallel data loading during model training. Essentially, this is helpful whenever you want to create batches of data and don't want to worry about handling the details!\n",
    "\n",
    "\n",
    "This **dataset** can be used as input to a [PyTorch Lightning DataModule](https://lightning.ai/docs/pytorch/stable/data/datamodule.html). The Lightning DataModule is an organized and encapsulated unit that abstracts the data loading, processing, and preparation for machine learning or deep learning models. It acts as a bridge between the raw data and the model, providing a structured interface to manage and handle datasets within the PyTorch Lightning framework.\n",
    "\n",
    "\n",
    "* Data Loading and Preparation: A DataModule handles the entire data pipeline, including loading datasets, splitting them into training, validation, and test sets, applying transformations, and preparing them for consumption by the model.\n",
    "\n",
    "* Reproducibility and flexibility : the modular organization facilitates reproducibility (making it easier to track data transformations) and reusability (making it easier to swap datasets or use the same data setup across different models)\n",
    "\n",
    "* Integration with PyTorch Lightning: DataModule seamlessly integrates with the PyTorch Lightning ecosystem, and can be fed directly into a PyTorch lightning trainer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "50d6e6d0-00eb-4be4-8a71-5908a1820faf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# The dataloader will convert our variables of interest into numpy arrays/tensors\n",
    "# and will through an error if it receives string variables\n",
    "# therefore we first encode our cell_type column into integers\n",
    "# (will through an error if you pass any string variables)\n",
    "\n",
    "ct_to_id_dict = {c : i for i, c in enumerate(adata.obs[\"cell_type\"].unique())}\n",
    "adata.obs[\"label\"] = adata.obs[\"cell_type\"].map(ct_to_id_dict)\n",
    "                                             "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afea9ebc-fe79-4655-b916-01aba0401f06",
   "metadata": {
    "tags": []
   },
   "source": [
    "`setup_simple_datamodule` return a PyTorch DataModule, that automatically sets up training/validation/test sets. For each batch, it returns a dictionary with an attribute `X` with adata.X if include_exprs = True, and any fields that were included in the setup_simple_datamodule arguments. For every value in obs_fields, it returns an item in the dataloader dictionary with a key of the format `obs_{column_name}` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "90f9cd78-3c85-41ed-b2da-07f897725ce1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/lustre/scratch126/cellgen/team292/mm58/venvs/scgpl_venv/lib/python3.10/site-packages/anndata/_core/anndata.py:121: ImplicitModificationWarning: Transforming to str index.\n",
      "  warnings.warn(\"Transforming to str index.\", ImplicitModificationWarning)\n"
     ]
    }
   ],
   "source": [
    "dm = setup_simple_datamodule(adata.copy(), \n",
    "                             train_frac=0.8,\n",
    "                             val_frac = 0.1,\n",
    "                             test_frac = 0.1, \n",
    "                             include_exprs=True, \n",
    "                             obs_fields=[\"label\"],\n",
    "                             batch_size = 32\n",
    "                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8aea79ae-1c44-4312-8e69-46ccc2c61cc1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X\n",
      "torch.Size([32, 1000])\n",
      "Encoded cell type label\n",
      "torch.Size([32])\n"
     ]
    }
   ],
   "source": [
    "for d in dm.train_dataloader():\n",
    "    print(\"X\")\n",
    "    print(d[\"X\"].shape)\n",
    "    print(\"Encoded cell type label\")\n",
    "    print(d[\"obs_label\"].shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7a7ac51a-84e7-4b8b-80af-e7cfa5c9c204",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X\n",
      "torch.Size([32, 1000])\n",
      "Encoded cell type label\n",
      "32\n",
      "torch.Size([32, 1000])\n"
     ]
    }
   ],
   "source": [
    "for d in dm.test_dataloader():\n",
    "    print(\"X\")\n",
    "    print(d[\"X\"].shape)\n",
    "    print(\"Encoded cell type label\")\n",
    "    print(len(d[\"obs_label\"]))\n",
    "    print(d[\"obs_label\"].shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31058189-2cb7-4051-92e9-5af28e9ed2b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cd250e2a-b05c-4541-93d6-c9b4824b0d36",
   "metadata": {},
   "source": [
    "<a id='mlp'></a>\n",
    "## Model 1: Multi-layer perceptron for cell classification\n",
    "\n",
    "As first example for how datamodules can work, we will build a simple neural network that takes gene expression data as input and outputs a probability for each cell type. This is an oversimplication, and mainly intended to provide a template for modules that can be replaced with more complex architectures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "460a27b1-045c-4194-8428-815a0eee7764",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dims, out_dim):\n",
    "        super().__init__()\n",
    "        \n",
    "        modules = []\n",
    "        for in_size, out_size in zip([input_dim]+hidden_dims, hidden_dims):\n",
    "            modules.append(nn.Linear(in_size, out_size))\n",
    "            modules.append(nn.LayerNorm(out_size))\n",
    "            modules.append(nn.ReLU())\n",
    "            modules.append(nn.Dropout(p=0.05))\n",
    "        modules.append(nn.Linear(hidden_dims[-1], out_dim))\n",
    "        self.fc = nn.Sequential(*modules)\n",
    "    \n",
    "    def forward(self, *inputs):\n",
    "        input_cat = torch.cat(inputs, dim=-1)\n",
    "        return self.fc(input_cat)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "24ffb02f-14bf-4e9f-a377-41ac99efb543",
   "metadata": {},
   "outputs": [],
   "source": [
    "class scLightning(pl.LightningModule):\n",
    "    \"\"\"\n",
    "    Pytorch lightning implementation of basic cell type classifier\n",
    "    \n",
    "    Inputs:\n",
    "    n_vars: \n",
    "        dimensions of input : this is the number of genes in slot X\n",
    "        \n",
    "    n_classes:\n",
    "        number of classes for prediction output (eg cell type labels)\n",
    "        \n",
    "    label_name:\n",
    "        name of the ground truth label that will be passed by the anndata module (here {obs/var}_{col_name})\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_vars, n_classes, label_col):\n",
    "        super().__init__()\n",
    "\n",
    "        self.n_classes = n_classes\n",
    "        \n",
    "        self.model = MLP(input_dim = n_vars, \n",
    "                         hidden_dims = [128, 64, 32], \n",
    "                         out_dim = self.n_classes)\n",
    "        \n",
    "\n",
    "        for stage in [\"train\", \"test\", \"val\"]:\n",
    "            setattr(self, f\"{stage}_loss\", [])\n",
    "            setattr(self, f\"{stage}_label_true\", [])\n",
    "            setattr(self, f\"{stage}_label_pred\", [])\n",
    "            \n",
    "        self.label_col = label_col\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=1e-3)\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x = batch[\"X\"]\n",
    "        y = batch[self.label_col].long()\n",
    "\n",
    "        out = self(x)\n",
    "        self.train_label_true.append(y)\n",
    "        self.train_label_pred.append(out)\n",
    "\n",
    "        loss = F.cross_entropy(out, y)\n",
    "\n",
    "        self.train_loss.append(loss)\n",
    "        self.log(f\"train/loss\", loss, prog_bar=True)\n",
    "\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x = batch[\"X\"]\n",
    "        y = batch[self.label_col].long()\n",
    "\n",
    "        out = self(x)\n",
    "        \n",
    "        self.val_label_true.append(y)\n",
    "        self.val_label_pred.append(out)\n",
    "\n",
    "        loss = F.cross_entropy(out, y)\n",
    "\n",
    "        self.val_loss.append(loss)\n",
    "        self.log(f\"val/loss\", loss, prog_bar=True)\n",
    "\n",
    "        return loss\n",
    "    \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        x = batch[\"X\"]\n",
    "        y = batch[self.label_col].long() \n",
    "        \n",
    "        print(\"x == y\", x == y)\n",
    "        \n",
    "        for k, v in batch.items():\n",
    "            print(k, \":\", v.shape)\n",
    "\n",
    "        \n",
    "        out = self(x)\n",
    "        \n",
    "        self.test_label_true.append(y)\n",
    "        \n",
    "        self.test_label_pred.append(out)\n",
    "\n",
    "        loss = F.cross_entropy(out, y)\n",
    "\n",
    "        self.test_loss.append(loss)\n",
    "        self.log(f\"test/loss\", loss, prog_bar=True)\n",
    "\n",
    "        return loss\n",
    "    \n",
    "    def on_train_epoch_end(self):\n",
    "        self.end_epoch(stage = \"train\")\n",
    "\n",
    "    def on_validation_epoch_end(self):\n",
    "        self.end_epoch(stage = \"val\")\n",
    "\n",
    "    def on_test_epoch_end(self):\n",
    "        self.end_epoch(stage = \"test\")\n",
    "\n",
    "    def end_epoch(self, stage):\n",
    "        epoch_loss = getattr(self, f\"{stage}_loss\")\n",
    "        epoch_loss = torch.stack(epoch_loss).sum()\n",
    "\n",
    "        epoch_label_true = getattr(self, f\"{stage}_label_true\")\n",
    "        epoch_label_true = torch.cat(epoch_label_true)\n",
    "\n",
    "        epoch_label_pred = getattr(self, f\"{stage}_label_pred\")\n",
    "        epoch_label_pred = torch.cat(epoch_label_pred)\n",
    "        epoch_label_pred = torch.argmax(epoch_label_pred, dim=-1)\n",
    "\n",
    "        # calculate accuracy \n",
    "        epoch_acc = (epoch_label_true == epoch_label_pred).float().mean()\n",
    "        self.log(f\"{stage}/acc\", epoch_acc, prog_bar=True)\n",
    "\n",
    "        # Reset \n",
    "        setattr(self, f\"{stage}_loss\", [])\n",
    "        setattr(self, f\"{stage}_label_true\", [])\n",
    "        setattr(self, f\"{stage}_label_pred\", [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9b0bec45-044c-4b46-a6aa-9370b7c41ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we first initialize a class \n",
    "model = scLightning(n_vars = adata.n_vars, \n",
    "                    n_classes=adata.obs[\"cell_type\"].nunique(), \n",
    "                    label_col = \"obs_label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "eee6b843-6802-4eac-a287-881ff96572bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "# define a pytorch trainer\n",
    "trainer = pl.Trainer(devices=-1, max_epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ddf063af-2dd6-453e-b9db-53e0932c64d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name  | Type | Params\n",
      "-------------------------------\n",
      "0 | model | MLP  | 139 K \n",
      "-------------------------------\n",
      "139 K     Trainable params\n",
      "0         Non-trainable params\n",
      "139 K     Total params\n",
      "0.557     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af081a00778149b1bfae94047fa148c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    }
   ],
   "source": [
    "# lightning handles all of the training\n",
    "trainer.fit(model, dm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d2c5eeff-0264-4191-8f7b-eec6d1d5ea30",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69136ade002248c6a573b1c50ea69154",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x == y tensor([[ True,  True, False,  ...,  True,  True,  True],\n",
      "        [ True,  True, False,  ...,  True,  True,  True],\n",
      "        [ True, False, False,  ...,  True,  True, False],\n",
      "        ...,\n",
      "        [False, False,  True,  ...,  True,  True,  True],\n",
      "        [False,  True,  True,  ...,  True,  True,  True],\n",
      "        [ True,  True, False,  ...,  True,  True,  True]], device='cuda:0')\n",
      "X : torch.Size([32, 1000])\n",
      "obs_label : torch.Size([32, 1000])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "0D or 1D target tensor expected, multi-target not supported",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[37], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# and testing\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdm\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/lustre/scratch126/cellgen/team292/mm58/venvs/scgpl_venv/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:735\u001b[0m, in \u001b[0;36mTrainer.test\u001b[0;34m(self, model, dataloaders, ckpt_path, verbose, datamodule)\u001b[0m\n\u001b[1;32m    733\u001b[0m     model \u001b[38;5;241m=\u001b[39m _maybe_unwrap_optimized(model)\n\u001b[1;32m    734\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39m_lightning_module \u001b[38;5;241m=\u001b[39m model\n\u001b[0;32m--> 735\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_and_handle_interrupt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    736\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_test_impl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatamodule\u001b[49m\n\u001b[1;32m    737\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/lustre/scratch126/cellgen/team292/mm58/venvs/scgpl_venv/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py:42\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     41\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher\u001b[38;5;241m.\u001b[39mlaunch(trainer_fn, \u001b[38;5;241m*\u001b[39margs, trainer\u001b[38;5;241m=\u001b[39mtrainer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m---> 42\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrainer_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _TunerExitException:\n\u001b[1;32m     45\u001b[0m     _call_teardown_hook(trainer)\n",
      "File \u001b[0;32m/lustre/scratch126/cellgen/team292/mm58/venvs/scgpl_venv/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:778\u001b[0m, in \u001b[0;36mTrainer._test_impl\u001b[0;34m(self, model, dataloaders, ckpt_path, verbose, datamodule)\u001b[0m\n\u001b[1;32m    773\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data_connector\u001b[38;5;241m.\u001b[39mattach_data(model, test_dataloaders\u001b[38;5;241m=\u001b[39mdataloaders, datamodule\u001b[38;5;241m=\u001b[39mdatamodule)\n\u001b[1;32m    775\u001b[0m ckpt_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_checkpoint_connector\u001b[38;5;241m.\u001b[39m_select_ckpt_path(\n\u001b[1;32m    776\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn, ckpt_path, model_provided\u001b[38;5;241m=\u001b[39mmodel_provided, model_connected\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    777\u001b[0m )\n\u001b[0;32m--> 778\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mckpt_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    779\u001b[0m \u001b[38;5;66;03m# remove the tensors from the test results\u001b[39;00m\n\u001b[1;32m    780\u001b[0m results \u001b[38;5;241m=\u001b[39m convert_tensors_to_scalars(results)\n",
      "File \u001b[0;32m/lustre/scratch126/cellgen/team292/mm58/venvs/scgpl_venv/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:973\u001b[0m, in \u001b[0;36mTrainer._run\u001b[0;34m(self, model, ckpt_path)\u001b[0m\n\u001b[1;32m    968\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_signal_connector\u001b[38;5;241m.\u001b[39mregister_signal_handlers()\n\u001b[1;32m    970\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m    971\u001b[0m \u001b[38;5;66;03m# RUN THE TRAINER\u001b[39;00m\n\u001b[1;32m    972\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[0;32m--> 973\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_stage\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    975\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m    976\u001b[0m \u001b[38;5;66;03m# POST-Training CLEAN UP\u001b[39;00m\n\u001b[1;32m    977\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m    978\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: trainer tearing down\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/lustre/scratch126/cellgen/team292/mm58/venvs/scgpl_venv/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:1009\u001b[0m, in \u001b[0;36mTrainer._run_stage\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1006\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mbarrier(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun-stage\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1008\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevaluating:\n\u001b[0;32m-> 1009\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_evaluation_loop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1010\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredicting:\n\u001b[1;32m   1011\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredict_loop\u001b[38;5;241m.\u001b[39mrun()\n",
      "File \u001b[0;32m/lustre/scratch126/cellgen/team292/mm58/venvs/scgpl_venv/lib/python3.10/site-packages/pytorch_lightning/loops/utilities.py:177\u001b[0m, in \u001b[0;36m_no_grad_context.<locals>._decorator\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    175\u001b[0m     context_manager \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mno_grad\n\u001b[1;32m    176\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context_manager():\n\u001b[0;32m--> 177\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mloop_run\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/lustre/scratch126/cellgen/team292/mm58/venvs/scgpl_venv/lib/python3.10/site-packages/pytorch_lightning/loops/evaluation_loop.py:115\u001b[0m, in \u001b[0;36m_EvaluationLoop.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    113\u001b[0m     previous_dataloader_idx \u001b[38;5;241m=\u001b[39m dataloader_idx\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;66;03m# run step hooks\u001b[39;00m\n\u001b[0;32m--> 115\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_evaluation_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader_idx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[1;32m    117\u001b[0m     \u001b[38;5;66;03m# this needs to wrap the `*_step` call too (not just `next`) for `dataloader_iter` support\u001b[39;00m\n\u001b[1;32m    118\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m/lustre/scratch126/cellgen/team292/mm58/venvs/scgpl_venv/lib/python3.10/site-packages/pytorch_lightning/loops/evaluation_loop.py:375\u001b[0m, in \u001b[0;36m_EvaluationLoop._evaluation_step\u001b[0;34m(self, batch, batch_idx, dataloader_idx)\u001b[0m\n\u001b[1;32m    372\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_progress\u001b[38;5;241m.\u001b[39mincrement_started()\n\u001b[1;32m    374\u001b[0m hook_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_step\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mtesting \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalidation_step\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 375\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_strategy_hook\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhook_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mstep_kwargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    377\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_progress\u001b[38;5;241m.\u001b[39mincrement_processed()\n\u001b[1;32m    379\u001b[0m hook_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mon_test_batch_end\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mtesting \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mon_validation_batch_end\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m/lustre/scratch126/cellgen/team292/mm58/venvs/scgpl_venv/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py:291\u001b[0m, in \u001b[0;36m_call_strategy_hook\u001b[0;34m(trainer, hook_name, *args, **kwargs)\u001b[0m\n\u001b[1;32m    288\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    290\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mprofile(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[Strategy]\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhook_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 291\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    293\u001b[0m \u001b[38;5;66;03m# restore current_fx when nested context\u001b[39;00m\n\u001b[1;32m    294\u001b[0m pl_module\u001b[38;5;241m.\u001b[39m_current_fx_name \u001b[38;5;241m=\u001b[39m prev_fx_name\n",
      "File \u001b[0;32m/lustre/scratch126/cellgen/team292/mm58/venvs/scgpl_venv/lib/python3.10/site-packages/pytorch_lightning/strategies/strategy.py:388\u001b[0m, in \u001b[0;36mStrategy.test_step\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    386\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprecision_plugin\u001b[38;5;241m.\u001b[39mtest_step_context():\n\u001b[1;32m    387\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel, TestStep)\n\u001b[0;32m--> 388\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtest_step\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[33], line 88\u001b[0m, in \u001b[0;36mscLightning.test_step\u001b[0;34m(self, batch, batch_idx)\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtest_label_true\u001b[38;5;241m.\u001b[39mappend(y)\n\u001b[1;32m     86\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtest_label_pred\u001b[38;5;241m.\u001b[39mappend(out)\n\u001b[0;32m---> 88\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtest_loss\u001b[38;5;241m.\u001b[39mappend(loss)\n\u001b[1;32m     91\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest/loss\u001b[39m\u001b[38;5;124m\"\u001b[39m, loss, prog_bar\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m/lustre/scratch126/cellgen/team292/mm58/venvs/scgpl_venv/lib/python3.10/site-packages/torch/nn/functional.py:3053\u001b[0m, in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   3051\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3052\u001b[0m     reduction \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[0;32m-> 3053\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_Reduction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_enum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: 0D or 1D target tensor expected, multi-target not supported"
     ]
    }
   ],
   "source": [
    "# and testing\n",
    "trainer.test(model, dm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4751bb78-b064-410b-9aa1-f96a08a457a3",
   "metadata": {
    "tags": []
   },
   "source": [
    "<a id='vae'></a>\n",
    "\n",
    "## Model 2: VAE implemtation in pytorch \n",
    "\n",
    "> note that this implementation has not been as thoroughly checked as scVI, and rather is intended to serve as an example of how we can use our datamodule for a more complicated models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba6020cb-c07b-4f19-bc0e-ff90aeb2f94f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# the scVI-style framework concatenates a one-hot-encoded vector representing the study/sample\n",
    "# for this we create a new obs column with our encoded study\n",
    "# and take care of one hot encoding inside the cVAE model\n",
    "study_to_index = {s : i for i, s in enumerate(adata.obs[\"study\"].unique().tolist())}\n",
    "\n",
    "adata.obs[\"encoded_study\"] = adata.obs[\"study\"].map(study_to_index)\n",
    "\n",
    "# we now define new datamodule that will pass the encoded_study and size_factors as well as labels \n",
    "dm_for_cvae = setup_simple_datamodule(adata.copy(), \n",
    "                                      train_frac=0.8, \n",
    "                                      include_exprs=True, \n",
    "                                      obs_fields=[\"label\", \"encoded_study\", \"size_factors\"],\n",
    "                                      batch_size = 32\n",
    "                                     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb0aeed1-c0f4-4a2d-9d0e-79fd56e8aebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Defining the cVAE model and custom trainer for interface with Pyro \n",
    "#### (the details here are not relevant to the datamodule class\n",
    "\n",
    "class LowLevelPyroTrainingPlan(pl.LightningModule):\n",
    "    \"\"\"Lightning module task to train Pyro scvi-tools modules.\n",
    "    from https://github.com/scverse/scvi-tools/blob/main/scvi/train/_trainingplans.py#L856\n",
    "    rewrite TunableMixin as new class property so class behaves as lightning module\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    pyro_module\n",
    "        An instance of :class:`~scvi.module.base.PyroBaseModuleClass`. This object\n",
    "        should have callable `model` and `guide` attributes or methods.\n",
    "    loss_fn\n",
    "        A Pyro loss. Should be a subclass of :class:`~pyro.infer.ELBO`.\n",
    "        If `None`, defaults to :class:`~pyro.infer.Trace_ELBO`.\n",
    "    optim\n",
    "        A Pytorch optimizer class, e.g., :class:`~torch.optim.Adam`. If `None`,\n",
    "        defaults to :class:`torch.optim.Adam`.\n",
    "    optim_kwargs\n",
    "        Keyword arguments for optimiser. If `None`, defaults to `dict(lr=1e-3)`.\n",
    "    n_steps_kl_warmup\n",
    "        Number of training steps (minibatches) to scale weight on KL divergences from 0 to 1.\n",
    "        Only activated when `n_epochs_kl_warmup` is set to None.\n",
    "    n_epochs_kl_warmup\n",
    "        Number of epochs to scale weight on KL divergences from 0 to 1.\n",
    "        Overrides `n_steps_kl_warmup` when both are not `None`.\n",
    "    scale_elbo\n",
    "        Scale ELBO using :class:`~pyro.poutine.scale`. Potentially useful for avoiding\n",
    "        numerical inaccuracy when working with very large ELBO.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        pyro_module: PyroBaseModuleClass,\n",
    "        loss_fn: Optional[pyro.infer.ELBO] = None,\n",
    "        optim: Optional[torch.optim.Adam] = None,\n",
    "        optim_kwargs: Optional[dict] = None,\n",
    "        n_steps_kl_warmup: Union[int, None] = None,\n",
    "        n_epochs_kl_warmup: Union[int, None] = 400,\n",
    "        scale_elbo: float = 1.0,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.module = pyro_module\n",
    "        self._n_obs_training = None\n",
    "\n",
    "        optim_kwargs = optim_kwargs if isinstance(optim_kwargs, dict) else {}\n",
    "        if \"lr\" not in optim_kwargs.keys():\n",
    "            optim_kwargs.update({\"lr\": 1e-3})\n",
    "        self.optim_kwargs = optim_kwargs\n",
    "\n",
    "        self.loss_fn = pyro.infer.Trace_ELBO() if loss_fn is None else loss_fn\n",
    "        self.optim = torch.optim.Adam if optim is None else optim\n",
    "        self.n_steps_kl_warmup = n_steps_kl_warmup\n",
    "        self.n_epochs_kl_warmup = n_epochs_kl_warmup\n",
    "        self.use_kl_weight = False\n",
    "        if isinstance(self.module.model, PyroModule):\n",
    "            self.use_kl_weight = (\n",
    "                \"kl_weight\" in signature(self.module.model.forward).parameters\n",
    "            )\n",
    "        elif callable(self.module.model):\n",
    "            self.use_kl_weight = \"kl_weight\" in signature(self.module.model).parameters\n",
    "        self.scale_elbo = scale_elbo\n",
    "        self.scale_fn = (\n",
    "            lambda obj: pyro.poutine.scale(obj, self.scale_elbo)\n",
    "            if self.scale_elbo != 1\n",
    "            else obj\n",
    "        )\n",
    "        self.differentiable_loss_fn = self.loss_fn.differentiable_loss\n",
    "        self.training_step_outputs = []\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        \"\"\"Training step for Pyro training.\"\"\"\n",
    "        args, kwargs = self.module._get_fn_args_from_batch(batch)\n",
    "        # Set KL weight if necessary.\n",
    "        # Note: if applied, ELBO loss in progress bar is the effective KL annealed loss, not the true ELBO.\n",
    "        if self.use_kl_weight:\n",
    "            kwargs.update({\"kl_weight\": self.kl_weight})\n",
    "        # pytorch lightning requires a Tensor object for loss\n",
    "        loss = self.differentiable_loss_fn(\n",
    "            self.scale_fn(self.module.model),\n",
    "            self.scale_fn(self.module.guide),\n",
    "            *args,\n",
    "            **kwargs,\n",
    "        )\n",
    "        out_dict = {\"loss\": loss}\n",
    "        self.training_step_outputs.append(out_dict)\n",
    "        return out_dict\n",
    "\n",
    "    def on_train_epoch_end(self):\n",
    "        \"\"\"Training epoch end for Pyro training.\"\"\"\n",
    "        outputs = self.training_step_outputs\n",
    "        elbo = 0\n",
    "        n = 0\n",
    "        for out in outputs:\n",
    "            elbo += out[\"loss\"]\n",
    "            n += 1\n",
    "        elbo /= n\n",
    "        self.log(\"elbo_train\", elbo, prog_bar=True)\n",
    "        self.training_step_outputs.clear()\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        \"\"\"Configure optimizers for the model.\"\"\"\n",
    "        return self.optim(self.module.parameters(), **self.optim_kwargs)\n",
    "\n",
    "    def forward(self, *args, **kwargs):\n",
    "        \"\"\"Passthrough to the model's forward method.\"\"\"\n",
    "        return self.module(*args, **kwargs)\n",
    "\n",
    "    @property\n",
    "    def kl_weight(self):\n",
    "        \"\"\"Scaling factor on KL divergence during training.\"\"\"\n",
    "        return _compute_kl_weight(\n",
    "            self.current_epoch,\n",
    "            self.global_step,\n",
    "            self.n_epochs_kl_warmup,\n",
    "            self.n_steps_kl_warmup,\n",
    "            min_kl_weight=1e-3,\n",
    "        )\n",
    "\n",
    "    @property\n",
    "    def n_obs_training(self):\n",
    "        \"\"\"Number of training examples.\n",
    "\n",
    "        If not `None`, updates the `n_obs` attr\n",
    "        of the Pyro module's `model` and `guide`, if they exist.\n",
    "        \"\"\"\n",
    "        return self._n_obs_training\n",
    "\n",
    "    @n_obs_training.setter\n",
    "    def n_obs_training(self, n_obs: int):\n",
    "        # important for scaling log prob in Pyro plates\n",
    "        if n_obs is not None:\n",
    "            if hasattr(self.module.model, \"n_obs\"):\n",
    "                self.module.model.n_obs = n_obs\n",
    "            if hasattr(self.module.guide, \"n_obs\"):\n",
    "                self.module.guide.n_obs = n_obs\n",
    "\n",
    "        self._n_obs_training = n_obs\n",
    "\n",
    "    @property\n",
    "    def _tunables(cls): #-> list[Any]:\n",
    "        \"\"\"Returns the tunable attributes of the model class.\"\"\"\n",
    "        _tunables = []\n",
    "        for attr_key in dir(cls):\n",
    "            if attr_key == \"_tunables\":\n",
    "                # Don't recurse\n",
    "                continue\n",
    "            attr = getattr(cls, attr_key)\n",
    "            if hasattr(attr, \"_tunables\") or isfunction(attr):\n",
    "                _tunables.append(attr)\n",
    "        return _tunables\n",
    "\n",
    "class cVAE(nn.Module):\n",
    "    # The code is based on the scarches trVAE model\n",
    "    # https://github.com/theislab/scarches/blob/v0.3.5/scarches/models/trvae/trvae.py\n",
    "    # and on the pyro.ai Variational Autoencoders tutorial\n",
    "    # http://pyro.ai/examples/vae.html\n",
    "    # adapted from https://github.com/scverse/anndata-tutorials/blob/main/annloader.ipynb\n",
    "    def __init__(self, input_dim, n_conds, n_classes, hidden_dims, latent_dim, study_to_index):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.encoder = MLP(input_dim+n_conds, hidden_dims, 2*latent_dim) # output - mean and logvar of z\n",
    "        \n",
    "        self.decoder = MLP(latent_dim+n_conds, hidden_dims[::-1], input_dim)\n",
    "        self.theta = nn.Linear(n_conds, input_dim, bias=False)\n",
    "        \n",
    "        self.classifier = nn.Linear(latent_dim, n_classes)\n",
    "        \n",
    "        self.latent_dim = latent_dim\n",
    "        \n",
    "        self.study_to_index = study_to_index\n",
    "    \n",
    "    def model(self, x, batches, classes, size_factors):\n",
    "        pyro.module(\"cvae\", self)\n",
    "        \n",
    "        batch_size = x.shape[0]\n",
    "        \n",
    "        with pyro.plate(\"data\", batch_size):\n",
    "            z_loc = x.new_zeros((batch_size, self.latent_dim))\n",
    "            z_scale = x.new_ones((batch_size, self.latent_dim))\n",
    "            z = pyro.sample(\"latent\", dist.Normal(z_loc, z_scale).to_event(1))\n",
    "            \n",
    "            classes_probs = self.classifier(z).softmax(dim=-1)\n",
    "            pyro.sample(\"class\", dist.Categorical(probs=classes_probs), obs=classes)\n",
    "            \n",
    "            dec_mu = self.decoder(z, batches).softmax(dim=-1) * size_factors[:, None]\n",
    "            dec_theta = torch.exp(self.theta(batches))\n",
    "            \n",
    "            logits = (dec_mu + 1e-6).log() - (dec_theta + 1e-6).log()\n",
    "            \n",
    "            pyro.sample(\"obs\", dist.NegativeBinomial(total_count=dec_theta, logits=logits).to_event(1), obs=x.int())\n",
    "    \n",
    "    def guide(self, x, batches, classes, size_factors):\n",
    "        batch_size = x.shape[0]\n",
    "        \n",
    "        with pyro.plate(\"data\", batch_size):\n",
    "            z_loc_scale = self.encoder(x, batches)\n",
    "            \n",
    "            z_mu = z_loc_scale[:, :self.latent_dim]\n",
    "            z_var = torch.sqrt(torch.exp(z_loc_scale[:, self.latent_dim:]) + 1e-4)\n",
    "            \n",
    "            pyro.sample(\"latent\", dist.Normal(z_mu, z_var).to_event(1))\n",
    "\n",
    "    def _get_fn_args_from_batch(self, batch):\n",
    "            #tensor_dict: dict[str, torch.Tensor]) -> Iterable | dict:\n",
    "            #usgae: args, kwargs = self.module._get_fn_args_from_batch(batch)\n",
    "        \"\"\"Parse the minibatched data to get the correct inputs for ``model`` and ``guide``.\n",
    "\n",
    "        In Pyro, ``model`` and ``guide`` must have the same signature. This is a helper method\n",
    "        that gets the args and kwargs for these two methods. This helper method aids ``forward`` and\n",
    "        ``guide`` in having transparent signatures, as well as allows use of our generic\n",
    "        :class:`~scvi.dataloaders.AnnDataLoader`.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        args and kwargs for the functions, args should be an Iterable and kwargs a dictionary.\n",
    "        \"\"\"\n",
    "\n",
    "        # split into x, batches, classes, size_factors\n",
    "        \n",
    "        # do the one hot encoding here \n",
    "        sample = torch.stack([self.one_hot_encode_study(batch[\"obs_encoded_study\"][i]) for i in range(len(batch[\"obs_encoded_study\"]))], \n",
    "                             dim = 0).to(torch.float32).to(batch[\"X\"].device)\n",
    "        \n",
    "        \n",
    "        return [batch[\"X\"], sample, batch[\"obs_label\"], batch[\"obs_size_factors\"]], {}\n",
    "    \n",
    "    def one_hot_encode_study(self, study_index):\n",
    "        study_index_int = int(study_index.item()) \n",
    "        num_classes = len(study_to_index)\n",
    "        return F.one_hot(torch.tensor(study_index_int), num_classes=num_classes)\n",
    "    \n",
    "def _compute_kl_weight(\n",
    "    epoch: int,\n",
    "    step: int,\n",
    "    n_epochs_kl_warmup: Optional[int],\n",
    "    n_steps_kl_warmup: Optional[int],\n",
    "    max_kl_weight: float = 1.0,\n",
    "    min_kl_weight: float = 0.0,\n",
    ") -> float:\n",
    "    \"\"\"Computes the kl weight for the current step or epoch.\n",
    "    from https://github.com/scverse/scvi-tools/blob/main/scvi/train/_trainingplans.py#L856\n",
    "\n",
    "    If both `n_epochs_kl_warmup` and `n_steps_kl_warmup` are None `max_kl_weight` is returned.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    epoch\n",
    "        Current epoch.\n",
    "    step\n",
    "        Current step.\n",
    "    n_epochs_kl_warmup\n",
    "        Number of training epochs to scale weight on KL divergences from\n",
    "        `min_kl_weight` to `max_kl_weight`\n",
    "    n_steps_kl_warmup\n",
    "        Number of training steps (minibatches) to scale weight on KL divergences from\n",
    "        `min_kl_weight` to `max_kl_weight`\n",
    "    max_kl_weight\n",
    "        Maximum scaling factor on KL divergence during training.\n",
    "    min_kl_weight\n",
    "        Minimum scaling factor on KL divergence during training.\n",
    "    \"\"\"\n",
    "    if min_kl_weight > max_kl_weight:\n",
    "        raise ValueError(\n",
    "            f\"min_kl_weight={min_kl_weight} is larger than max_kl_weight={max_kl_weight}.\"\n",
    "        )\n",
    "\n",
    "    slope = max_kl_weight - min_kl_weight\n",
    "    if n_epochs_kl_warmup:\n",
    "        if epoch < n_epochs_kl_warmup:\n",
    "            return slope * (epoch / n_epochs_kl_warmup) + min_kl_weight\n",
    "    elif n_steps_kl_warmup:\n",
    "        if step < n_steps_kl_warmup:\n",
    "            return slope * (step / n_steps_kl_warmup) + min_kl_weight\n",
    "    return max_kl_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4e7e31d-be4f-482d-99c5-54866b853556",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = cVAE(input_dim=adata.n_vars, \n",
    "             n_conds=adata.obs[\"study\"].nunique(), \n",
    "             n_classes=adata.obs[\"cell_type\"].nunique(), \n",
    "             hidden_dims=[128, 64, 32], \n",
    "             latent_dim=10, \n",
    "             study_to_index = study_to_index\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8afdf500-86ce-4d46-9ec6-58a2595276ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "pyro_model = LowLevelPyroTrainingPlan(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d194bd7a-71a2-4cd1-9ef7-b71163b12b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = pl.Trainer(devices=-1, max_epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c458725-164d-4b3f-9640-00cd924c0f9d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "trainer.fit(pyro_model, dm_for_cvae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b076704b-ed4b-49fe-aaf9-6066e006ce06",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_latent_representation(model, dm, study_to_index):\n",
    "    \"\"\"\n",
    "    Extracts the latent representation from the trained cVAE from the given model using data from DataModule (dm).\n",
    "\n",
    "    Args:\n",
    "    - model (cVAE): The model from which the latent representation means are extracted.\n",
    "    - dm : a data module of class SimpleDataModule(LightningDataModule)\n",
    "    - study_to_index (dict): A dictionary mapping study names to corresponding indices.\n",
    "\n",
    "    Returns:\n",
    "    - numpy.ndarray: An array containing the mean values of the latent variables for each sample.\n",
    "    \"\"\"\n",
    "    \n",
    "    def one_hot_encode_study(study, study_to_index):\n",
    "        study_index = study_to_index[study]\n",
    "        num_classes = len(study_to_index)\n",
    "        return F.one_hot(torch.tensor(study_index), num_classes=num_classes)\n",
    "    \n",
    "    \n",
    "    dm.setup()\n",
    "    full_data = dm.dataset[:] # No copies yet, nothing is copied until you access specific attributes (.X, .obsm etc.).\n",
    "    \n",
    "    studyx = torch.stack([one_hot_encode_study(s, dm.study_to_index) for s in full_data[\"obs\"].study], dim = 0).to(torch.float32)\n",
    "\n",
    "    means = model.encoder(torch.tensor(full_data[\"X\"], dtype = torch.float32), \n",
    "                          studyx)[:, :10] # get mean values of the latent variables\n",
    "\n",
    "    return means.data.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69a8ed1f-1b74-4f9c-ac58-275200a42e56",
   "metadata": {},
   "outputs": [],
   "source": [
    "adata.obsm['X_cvae'] = get_latent_representation(model, dm, study_to_index)\n",
    "\n",
    "sc.pp.neighbors(adata, use_rep='X_cvae')\n",
    "sc.tl.umap(adata)\n",
    "\n",
    "sc.pl.umap(adata, color=['study', 'cell_type'], wspace=0.35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30135629-6d20-498d-8836-9286901c9de2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "scgpl_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
